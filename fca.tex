\documentclass[11pt]{article}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[normalem]{ulem}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\begin{document}

% ========== Edit your name here
\author{Simone Montali - monta.li}
\title{Fondamenti di Controlli Automatici}

\maketitle

\medskip
\section{Il controllo attivo di un processo}
Innanzitutto definiamo due termini fondamentali per la materia: un \textbf{processo} è l'evoluzione nel tempo di ciò che caratterizza un sistema. Con \textbf{controllo attivo} intendiamo una strategia di controllo che prevede un'azione di comando esercitata sul processo. 
Il controllo attivo risolve il problema di imporre una modalità di funzionamento desiderato ad un processo: l'obiettivo è che una variabile del processo coincida con una preassegnata. Parliamo di \textbf{regolazione} quando l'ingresso è costante, di \textbf{asservimento} quando l'ingresso è variabile. 
Un \textbf{sistema} è un complesso, normalmente composto da più elementi interconnessi, in cui si possono distinguere grandezze soggette a variare nel tempo (variabili). Un \textbf{segnale} è una funzione che rappresenta l'andamento delle variabili nel tempo. Distinguiamo queste ultime in indipendenti (ingressi) e dipendenti (uscite). Arriviamo così al concetto di \textbf{sistema orientato}. Un \textbf{modello matematico} è la descrizione di un sistema che permette di determinare i segnali delle uscite noti gli ingressi e le condizioni iniziali. Distinguiamo tra sistemi multivariabili (MIMO) e scalari (SISO). Un sistema è detto \textbf{statico} quando l'uscita al tempo $t$ dipende esclusivamente dall'ingresso al medesimo tempo $t$. Un \textbf{sistema dinamico}, invece, ha uscita dipendente dal segnale di ingresso sull'intervallo $(-\infty , t]$, e ha quindi memoria. Per questi ultimi sistemi introduciamo i concetti di sistema in quiete (\textit{equilibrio}) e sistema in condizioni asintotiche(\textit{stazionarie}). 
\subsection{Insieme dei behavior}
Definiamo ora l'\textbf{insieme dei behavior} $\mathcal{B}$ come l'insieme di tutte le possibili coppie causa-effetto associate ad un sistema.
\begin{displaymath}
    \mathcal{B}:={\left( u(t), y(t) \right) : y(t)}
\end{displaymath}
è l'uscita del sistema corrispondente all'ingresso $u(t)$, con $u(t)$ e $y(t)$ che tipicamente appartengono agli spazi funzionali delle funzioni continue o differenziabili a tratti. 
Un sistema è \textbf{lineare} se soddisfa la proprietà di sovrapposizione degli effetti. 
\begin{displaymath}
    \forall (u_1,y_1), (u_2, y_2) \in \mathcal{B}, \forall \alpha_1,\alpha_2 \in \mathcal{R} \rightarrow \alpha_1(u_1,y_1)+\alpha_2(u_2, y_2) := (\alpha_1 u_1+ \alpha_2 u_2, \alpha_1y_1 + \alpha_2 y_2) \in \mathcal{B}
\end{displaymath}
Con \textbf{stazionario} intendiamo un sistema invariante nel tempo, ossia:
\begin{displaymath}
    \left(u(t), y(t)\right) \in \mathcal{B} \rightarrow \left(u(t-T), y(t-T)\right) \in \mathcal{B}
\end{displaymath}
\subsection{Controllo ad azione diretta e retroazione}
Vi è, tra le tipologie di controllo, una distinzione importantissima: quella tra i controlli \textbf{ad azione diretta} e quelli \textbf{in retroazione}. 
Nel primo, l'azione di comando dipende da: obiettivo perseguito, informazioni sul modello del sistema controllato, ingressi agenti sul sistema controllato. Nel secondo, oltre ai suddetti, vi è l'intervento della \textbf{variabile controllata}. In altri termini, l'ingresso dipende anche dall'uscita. Introduciamo poi anche i controlli feedforward/feedback a due/tre gradi di libertà. 
È utile notare come in sistemi disturbati, in cui cioè abbiamo una perturbazione data dal sistema stesso, il controllo ad azione diretta non la smorza, mentre quello in retroazione riduce l'errore di svariati ordini di grandezza. Bisogna però portare attenzione ai \textbf{fenomeni di instabilità} che nascono all'aumentare del guadagno di anello. 
\section{Modellistica ed equazioni differenziali lineari}
\subsection{Cenni di modellistica}
La \textbf{modellistica} è la costruzione dei modelli matematici dei sistemi, a partire da leggi fondamentali o dati sperimentali.
\subsubsection{Circuiti elettrici}
Citiamo anzitutto alcuni esempi elettrici di leggi fondamentali:\\
\begin{center}
Resistenza: $v_R = Ri$\\
Induttanza: $v_L=L\frac{di}{dt} = LDi$\\
Capacità: $v_c = \frac{Q}{C} = \frac{1}{C} \int_{-\infty}^t i(\tau)d\tau \rightarrow Dv_c = \frac{i}{C}$\\
\end{center}

Un circuito RLC diventa quindi
\begin{displaymath}
    v_i=v_L+v_R+v_c\\
    v_i(t) = LDi(t) + Ri(t)+ \frac{1}{C} \int_{-\infty}^t i(\tau)d\tau
\end{displaymath}
Calcoliamo quindi l'equazione differenziale lineare a coefficienti costanti:
\begin{displaymath}
    LD^2i+RDi+\frac{1}{C}i = Dv_i
\end{displaymath}
Costruiamo il modello matematico orientato da $v_i$ a $v_u$.
\begin{displaymath}
    LCD^2 v_u + RCDv_u + v_u = v_i
\end{displaymath}
\subsubsection{Sistemi meccanici}
Citiamo ora tre sistemi meccanici e le rispettive leggi del moto.
\begin{center}
    Massa: $MD^2x(t) = f_1(t) - f_2(t)$\\
    Molla: $f(t)=K(x_1(t)-x_2(t))$\\
    Ammortizzatore: $f(t) = B(v_1(t)-v_2(t)) \hspace{20px} f(t) = BD(x_1(t)-x_2(t))$
\end{center}
Introduciamo un sistema meccanico vibrante composto dai tre elementi, che avrà quindi equazione da f a x:
\begin{displaymath}
    mD^2 x(t) + bDx(t) + kx(t) = f(t)
\end{displaymath}
Otteniamo l'equazione differenziale
\begin{displaymath}
    mD^2y+bDy+ky = Df
\end{displaymath}
\subsubsection{OP-AMP}
Citiamo anche i circuiti elettrici con OP-AMP, deducendo 
\begin{displaymath}
    R_1CDy + y = -R_2CDu-u
\end{displaymath}
\subsection{Equazioni differenziali lineari}
Le equazioni differenziali lineari a coefficienti costanti possono rappresentare quindi sistemi scalari, generalizzando così:
\begin{displaymath}
    \sum_{i=0}^n a_i D^i y = \sum_{i=0}^m b_i D^i u
\end{displaymath}
Otteniamo così un modello matematico formale del sistema dinamico (orientato) $\Sigma, y =$ variabile d'uscita, $u=$ variabile d'ingresso, $a_n \neq 0, b_m \neq 0$. $n$ è l'ordine dell'eq. differenziale per estensione ordine di $\Sigma$, $n\ge m$; $\rho := n-m$ ordine relativo o grado relativo di $\Sigma$.
Ricollegandoci al concetto di \textbf{insieme dei behaviors} $\mathcal{B}$ di $\Sigma$, la coppia di segnali $\mathcal{B} := \{\left(u(t), y(t)\right)$ soddisfa l'equazione differenziale se $u(t)$ e $y(t)$ sono derivabili tante volte quanto necessario. 
\subsubsection{Proprietà del sistema}
Citiamo allora alcune proprietà del sistema. Per le dimostrazioni riferirsi alle slide.
\begin{itemize}
    \item Il sistema è lineare.
    \item Il sistema è stazionario.
\end{itemize}
\subsection{Determinazione dei segnali di uscita}
Una volta introdotto il sistema, sorge spontaneo un dubbio fondamentale:
\begin{center}
    \textbf{Noto il segnale di ingresso $u(t)|_{[0, +\infty]}$ e le condizioni iniziali $y(0), Dy(0),...,D^{n-1}y(0)$ determinare il segnale di uscita $y(t)|_{[0, +\infty]}$}
\end{center}
Se avessimo un'equazione omogenea, la soluzione sarebbe immediata. Ma spesso non è così, e ci serve un metodo generale per poter trattare le diverse casistiche. 
La classe dei segnali che utilizzeremo è $C_p^\infty$, insieme delle funzioni infinitamente derivabili a tratti. 
\begin{center}
    \textit{Una funzione appartiene a $C_p^\infty$ se esiste un insieme sparso $\mathcal{S}$ per il quale $f \in C^\infty (\mathbb{R}/S, \mathbb{R})$ e per ogni $n \in \mathbb{N}$ e per ogni $t \in \mathcal{S}$ i limiti $f^{(n)}(t^-)$ e $f^{(n)}(t^+)$ esistono e sono finiti. Quando $f$ è definita in $t \in \mathcal{S}$, convenzionalmente $f(t) := f(t^+)$. In particolare $C^{-1} := C_p^\infty (\mathbb{R})$ definisce l'insieme delle funzioni di classe $C^\infty$ a tratti definite su tutto $\mathbb{R}$}
\end{center}
\subsubsection{Proprietà di $C_p^\infty$}
In generale, sappiamo che $C^k \not\subset C^\infty_p$ e che $C_p^{k,\infty} := C^k \cap C^\infty_p$.
\subsubsection{Grado di continuità di una funzione o segnale}
Definiamo il grado di continuità di una funzione o segnale:
\begin{center}
    \textit{Se $f \in C_p^{k,\infty}, k $ è il grado di continuità di $f$.}
\end{center}
\begin{displaymath}
    \overline{C_p^{k,\infty}} := \left\{ f:\mathbb{R} \rightarrow \mathbb{R}:f \in C_p^{k, \infty}  \wedge f \not\in C_p^{k+1,\infty}\right\}
\end{displaymath}
Se $f \in \overline{C_p^{k,\infty}}$ allora $k$ è il grado massimo di continuità di $f$.
\subsubsection{Trasformate di Laplace}
Il metodo generale proposto per "integrare" l'equazione differenziale di $\Sigma$ si basa sulla \textbf{trasformata di Laplace}, che permette di trasformare un'equazione differenziale in un'equazione algebrica.
\section{Cenni di analisi complessa}
\subsection{Limite di una funzione complessa}
Consideriamo una funzione complessa di variabile complessa $f: \mathbb{C} \rightarrow \mathbb{C} s \rightarrow f(s)$. Se $s = \sigma + j \omega$, allora $f(s) = u(\sigma, \omega)+ jv(\sigma, \omega)$. Definiamo il limite $lim_{s\rightarrow s_0} f(s) = \lambda$ con la classica definizione da Analisi 1:
\begin{center}
    $\forall \epsilon >0, \exists \rho >0$ tale che se $s$ soddisfa $0<|s-s_0|<\rho \rightarrow |f(s) - \lambda| < \epsilon$
\end{center}
\subsubsection{Altre proprietà derivate}
Data questa definizione, possiamo definire altre proprietà, come la \textbf{continuità}: $f$ è continua in $s=s_0$ se $lim_{s\rightarrow s_0} f(s) = f(s_0)$. Da qui, $f(s)$ è continua in $s_0 = \sigma_0 + j\omega_0$ sse le funzioni reali $u(\sigma, \omega), v(\sigma, \omega)$ sono continue in $(\sigma_0, \omega_0)$. Definiamo poi la \textbf{derivabilità}: $f(s)$ è derivabile in $s=s_0$ se esiste il limite 
\begin{displaymath}
    lim_{\Delta s\rightarrow 0} \frac{f(s_0 + \Delta s) - f(s_0)}{\Delta s}
\end{displaymath}
Le regole base di derivabilità dell'analisi rimangono valide. Definiamo l'\textbf{analiticità}, ossia, $f(s)$ è analitica/olomorfa in $s=s_0$ se $f(s)$ è derivabile su di un intorno aperto contenente $s_0$. 
Infine, definiamo le \textbf{condizioni di Cauchy-Riemann}: $u(\sigma, \omega), v(\sigma, \omega)$ soddisfano le condizioni di Cauchy-Riemann se 
\begin{displaymath}
    \begin{cases} 
        \frac{\partial u}{\partial \sigma} = \frac{\partial v}{\partial \omega} \\ \frac{\partial u}{\partial \omega} = - \frac{\partial v}{\partial \sigma}
    \end{cases}
\end{displaymath}
\paragraph{Teorema}
Sia $f(s) = u(s) + jv(s)$:
\begin{enumerate}
    \item Se esiste $f^{(1)} (s_0)$ con $s_0 = \sigma_0 + j\omega_0$ allora esistono le derivate parziali di $u(\sigma, \omega), v(\sigma, \omega)$ in $(\sigma_0, \omega_0)$ e soddisfano le equazioni di Cauchy-Riemann
    \item Se $u(\sigma, \omega), v(\sigma, \omega)$ e le loro derivate parziali sono continue in $(\sigma_0, \omega_0)$ e soddisfano le condizioni di Cauchy-Riemann, allora esiste $f^{(1)} (s_0)$ con $s_0 = \sigma_0 + j\omega_0$
\end{enumerate}
\paragraph{Corollario}
Sia $f(s) = u(s) + jv(s)$ con $u(\sigma, \omega), v(\sigma, \omega)$ e le loro derivate parziali continue su di un dominio aperto $U \subseteq C$. Allora $f(s)$ è analitica su $U$ se e solo se $u(\sigma, \omega), v(\sigma, \omega)$ soddisfano, su U, le condizioni di Cauchy-Riemann. 
\paragraph{Teorema} 
Sia $f(s)$ analitica su di una regione aperta $U \subseteq C$. Allora la derivata $Df(s)$ è anch'essa una funzione analitica su $U$.
\paragraph{Corollario}
Se $f(s)$ è analitica sulla regione aperta $U$, allora $f(s)$ è ivi derivabile indefinitivamente.
\subsection{Integrali di linea nel piano complesso}
Definiamo innanzitutto l'\textbf{integrale}: data una funzione $f(s)$ ed una curva $\Gamma$ su $\mathbb{C}$ percorsa da $s_a$ a $s_b$, definiamo $\int_\Gamma f(s) ds \triangleq lim_{n\rightarrow \infty} \sum_{i=1}^n f(s_i)(s_i- s_{i-1})$ dove $s_0,...,s_n$ è una discretizzazione uniforme della curva $\Gamma$. 
\subsubsection{Calcolo dell'integrale di linea}
Sia $\Gamma$ una curva parametrica di classe $C^1$.
\begin{displaymath}
    \int_\Gamma f(s) ds = \int_a^b f\left(\Gamma(u) \right) \frac{d\Gamma}{du} du
\end{displaymath}

Definiamo una \textbf{curva chiusa semplice} come una curva continua tale che $\Gamma(a)=\Gamma(b)$ e $\Gamma(u_1) \neq \Gamma(u_2) \forall u_1 \neq u_2 \in (a,b)$
Il \textbf{teorema di Jordan} afferma che se $\Gamma$ è una curva chiusa semplice in $\mathbb{C}$ questa suddivide il piano complesso in due regioni distinte, una esterna e una interna. 
Definiamo un \textbf{insieme connesso} se per ogni coppia di punti appartenenti all'insieme esiste una curva continua $\Gamma$ che li congiunge tutta contenuta in R. È invece detto \textbf{semplicemente connesso} se è connesso e per ogni curva chiusa semplice $\Gamma$ appartenente all'insieme la regione interna di $\Gamma$ è tutta contenuta in R.
\paragraph{Teorema dell'integrale di Cauchy}
Sia $f(s)$ una funzione analitica su di una regione aperta e semplicemente connessa $U$ e $\Gamma$ una curva semplice ivi contenuta. Allora $\oint_\Gamma f(s) ds = 0$. 
\paragraph{Corollario}
Sia $f(s)$ una funzione analitica su di una regione aperta e semplicemente connessa $U$ e $\Gamma$ una curva ivi contenuta che congiunge $s_a$ ad $s_b$. Allora l'integrale di linea $\int_\Gamma f(s)ds$ non dipende dal percorso $\Gamma$ ma solo da $s_a, s_b$ e $f(s)$:
\begin{displaymath}
    \int_\Gamma f(s)ds = \int_{s_a}^{s_b} f(s)ds
\end{displaymath}
\paragraph{Teorema - Sviluppo in serie di Taylor}
Sia $f(s)$ una funzione analitica su di un cerchio $B(s_0, r_0)$ centrato su $s_0$ e con raggio $r_0$. Allora $\forall s \in B(s_0, r_0)$ 
\begin{displaymath}
    f(s) = \sum_{i=0}^\infty c_i (s-s_0)^i
\end{displaymath}
dove 
\begin{displaymath}
    c_i = \frac{f^{(i) (s_0)}}{i!} = \frac{1}{2\pi j}\oint \frac{f(s)}{(s-s_0)^{i+1}}ds
\end{displaymath}
Come corollario, otteniamo la \textbf{formula integrale di Cauchy}
\begin{displaymath}
    f(s_0) = \frac{1}{2\pi j} \oint_\Gamma \frac{f(s)}{s-s_0} ds
\end{displaymath}
\paragraph{Teorema - Sviluppo in serie di Laurent}
Sia $f(s)$ una funzione analitica sul cerchio $B(s_0, r_0)$ ad eccezione del suo centro $s_0$. Allora $\forall s \in B(s_0, r_0)- \{s_0\}$
\begin{displaymath}
    f(s) = \sum_{i=-\infty}^{+\infty} c_i(s-s_0)^i
\end{displaymath}
dove 
\begin{displaymath}
    c_i = \frac{f^{(i) (s_0)}}{i!} = \frac{1}{2\pi j}\oint \frac{f(s)}{(s-s_0)^{i+1}}ds
\end{displaymath}
\subsection{Classificazione del punto isolato $s_0$}
Se $c_i =0 \forall i \in \mathbb{Z}^-$ definendo $f(s_0) = c_0$ risulta $f(s)$ analitica in $s=s_0$. Se $c_i \neq 0$ per qualche $i \in \mathbb{Z}^-$, $s_0$ è una singolarità di $f(s)$, detta \textbf{singolarità polo} quando i $c_i \neq 0$ sono in numero finito, con $-n = min\{i\in \mathbb{Z}^-: c_i \neq 0\}$ $s_0$ è polo di ordine $n$. Invece abbiamo una \textbf{singolarità essenziale} quando i $c_i \neq 0$ con $i \in \mathbb{Z}^-$ sono infiniti. Se abbiamo una $f(s)$ analitica in $B(s_0, r_0) - \{s_0\}$, $s_0$ è una singolarità di $f(s)$ se e solo se $f(s)$ assume valori illimitati in un intorno di $s_0$. 
Il \textbf{Teorema di Picard} afferma che se abbiamo $s_0$ singolarità essenziale di $f(s)$, in ogni intorno di $s_0$ la funzione $f(s)$ assume ogni valore complesso infinite volte con l'eventuale eccezione di un solo particolare valore. 
\subsubsection{Residui}
Data una singolarità $s_0$, definiamo il \textbf{residuo} come il coefficiente $c_{-1}$ dello sviluppo in serie di Laurent.
\paragraph{Teorema dei residui di Cauchy}
Sia $\Gamma$ una curva chiusa semplice e $f(s)$ una funzione analitica su $\Gamma$ e nella sua regione interna ad eccezione dei punti singolari $s_1,...,s_n$ in essa contenuti, allora 
\begin{displaymath}
    \oint_\Gamma f(s) ds = 2\pi j \sum_{i=1}^n Res\{f, s_i\}
\end{displaymath}
\subsubsection{Poli e zeri}
Sia $s_0$ una singolarità polo di $f(s)$. Allora $s_0$ è polo di ordine $n$ sse esiste $g(s)$ analitica in $s_0$ con $g(s_0) \neq 0$ tale che 
\begin{displaymath}
    f(s) = \frac{g(s)}{(s-s_0)^n}
\end{displaymath}
Sia $f(s)$ analitica in $z$. $z$ è detto \textbf{zero} di $f$ se $f(z)=0$. Considerato lo sviluppo di Taylor $f(s)= c_1(s-z)+\dots$ ed $n:=min\{i \in \mathbb{N}: c_i \neq 0\}$, $z$ è detto zero di ordine $n$ di $f(s)$. È detto tale se e solo se esiste $g(s)$ analitica in $z$ con $g(z) \neq 0$ tale che $f(s) = (s-z)^n g(s)$. Ricaviamo un'ultima proprietà: se $f: \mathbb{C} \rightarrow \mathbb{C}$ ha una singolarità polare in $p$ di ordine $n$ allora 
\begin{displaymath}
    Res\{f,p\} = \frac{1}{(n-1)!} D^{n-1} \left(f(s)(s-p)^n\right)|_{s=p}
\end{displaymath}
\subsection{Continuazione analitica}
Data una funzione $f(s)$ definita da uno sviluppo in serie di Taylor su di un cerchio $B_0(s_0, r_0)$ è possibile estendere/continuare la definizione di $f(s)$ all'esterno di $B_0$ mediante lo sviluppo in serie di Taylor di altri punti di $B_0$. Il procedimento è ricorsivo. Possono anche emergere funzioni a più valori!
\section{La trasformata di Laplace}
La \textbf{trasformata di Laplace} è un operatore funzionale che converte un'equazione differenziale in un'equazione algebrica, permettendo di risolvere anche equazioni differenziali lineari con condizioni iniziali arbitrarie.
Permette inoltre di analizzare i fenomeni transitori ed asintotici di una grande varietà di sistemi. 
Si applica ad una funzione $f$ di variabile reale con codominio $\mathbb{R}$ o $\mathbb{C}$. Assumiamo ora $f \in \mathbb{C}_p^\infty (\mathbb{R})$, sappiamo che $\exists \sigma \in \mathbb{R}$ per il quale $\int_0^{+\infty} |f(t)| e^{-\sigma t} dt < + \infty$. Se vale quest'ultima condizione, allora $\forall \sigma_1 > \sigma : \int_0^{+\infty} |f(t)| e^{-\sigma_1 t} dt < + \infty$. Definiamo l'\textbf{ascissa di convergenza} di $f(t)$ come $\sigma_c := inf \left\{\sigma \in \mathbb{R} : \int_0^{+\infty}|f(t)| e^{- \sigma t} dt\right\}$ Spesso assumeremo $f(t) = 0$ per $t<0$. 
Volendo ora dare una definizione rigorosa della trasformata, 
\begin{center}
    La trasformata di Laplace di un segnale (funzione) $f(t)$ è la funzione $F(s) = \mathcal{L}[f(t)]$ definita da
    \begin{displaymath}
        F(s) = \int_0^{+\infty} f(t) e^{-st} dt
    \end{displaymath}
    per i valori $s\in \mathbb{C}$ per i quali l'integrale converge.
\end{center}
Notiamo alcune cose:
\begin{itemize}
    \item $F$ è una funzione complessa di variabile complessa, $\mathcal{L}[\dot]$ indica l'operatore di Laplace.
    \item La notazione usuale prevede che le lettere minuscole denotino segnali e funzioni, le corrispondenti maiuscole le loro trasformate.
\end{itemize}
\subsection{Proprietà della trasformata}
\subsubsection{Analitica}
La trasformata $F(s)$ è una funzione analitica sul semipiano $\left\{s \in \mathbb{C}: Re s > \sigma_c\right\}$
\subsubsection{Coniugato}
Denotando il coniugato con $\overline{ }$, $\overline{F(s)} = F(\overline{s})$
\subsubsection{Linearità}
La trasformata di Laplace è un operatore lineare: per ogni segnale $f_1(t)$ e $f_2(t)$ e per ogni scalare $c_1$ e $c_2$:
\begin{displaymath}
    \mathcal{L}[c_1 f_1 (t) + c_2f_2(t)] = c_1 \mathcal{L}[f_1(t)] + c_2 \mathcal{L}[f_2(t)]
\end{displaymath}
\subsubsection{Iniettività}
La trasformata di Laplace è \textbf{iniettiva}:
\begin{displaymath}
    \mathcal{L}[f(t)] = \mathcal{L}[g(t)] \rightarrow f(t) = g(t) \textrm{ su }[0, +\infty )
\end{displaymath}
È quindi ben definita la trasformata inversa.
\subsection{La trasformata inversa di Laplace}
Sia $F(s) = \mathcal{L}[f(t)]$ allora, per ogni $\sigma_0 > \sigma_c$
\begin{displaymath}
    \mathcal{L}^{-1} \rightarrow f(t) = \frac{1}{2\pi j} \int_{\sigma_0-j\infty}^{\sigma_0+j\infty} F(s) e^{st} ds
\end{displaymath}
\subsection{Trasformate notevoli}
\subsubsection{Trasformata della derivata}
Sia $f \in C^1 (\mathbb{R}_{>0})$ segue
\begin{displaymath}
    \mathcal{L}[Df(t)] = sF(s) - f(0+)
\end{displaymath}
Generalizzando per gli ordini superiori, otteniamo
\begin{displaymath}
    \mathcal{L}[D^i f] = s^i F(s) - \sum_{j=0}^{i-1} s^j D^{i-1-j} f_+
\end{displaymath}
\subsubsection{Trasformata dell'integrale}
\begin{displaymath}
    \mathcal{L}\left[\int_0^t f(v) dv\right] = \frac{1}{s} F(s)
\end{displaymath}
\subsection{Teoremi e gotchas}
\subsubsection{Teorema del valore finale}
Sia $f\in C^1 (\mathbb{R}_+)$ con $f$ e $Df$ aventi ascisse di convergenza non positive. Se esiste il limite $lim_{t\rightarrow+\infty} f(t)$ vale
\begin{displaymath}
    lim_{t\rightarrow+\infty} f(t) = lim_{s\rightarrow0} sF(s)
\end{displaymath}
\subsubsection{Teorema del valore iniziale}
Sia $f\in C^1 (\mathbb{R}_+)$. Se esiste il limite $lim_{s\rightarrow+\infty} sF(s)$ vale
\begin{displaymath}
    f(0+)=lim_{s\rightarrow+\infty} sF(s)
\end{displaymath}
\subsubsection{Traslazione nel tempo}
Per ogni $t_0 \ge 0$ vale 
\begin{displaymath}
    \mathcal{L}[f(t-t_0)\cdot 1(t-t_0)]= e^{-t_0s}F(s)
\end{displaymath}
\subsubsection{Traslazione nella variabile complessa $s$}
Per ogni $a \in \mathbb{R}(\mathbb{C})$ vale 
\begin{displaymath}
    \mathcal{L}[e^{\alpha t} f(t)] = F(s-a)
\end{displaymath}
\subsubsection{Teorema di convoluzione}
Si abbia $f(t) = g(t)=0$ per $t<0$. La convoluzione dei segnali $f$ e $g$, spesso indicata come $f*g$, è il segnale 
\begin{displaymath}
    \int_0^t f(v)g(t-v)dv
\end{displaymath}
rappresentabile anche come $f*g=g*f$
\begin{displaymath}
    \int_0^t g(v) f(t-v) dv
\end{displaymath}
La trasformata della convoluzione è 
\begin{displaymath}
    \mathcal{L}\left[    \int_0^t f(v)g(t-v)dv\right] = F(s)G(s)
\end{displaymath}
\subsection{Antitrasformazione di funzioni razionali}
Per antitrasformare le funzioni razionali, sfruttiamo il \textbf{metodo dei fratti semplici}, ossia scomponiamo il denominatore in poli semplici e poi cerchiamo i $k_i$:
\begin{displaymath}
    F(s) = \frac{k_1}{(s-p_1)}+\frac{k_2}{(s-p_2)}+...+\frac{k_n}{(s-p_n)}
\end{displaymath}
Con i $k_i$ rappresentanti il residuo di $F(s)$ in $p_i$, e pari a
\begin{displaymath}
    k_i= (s-p_i)F(s)|_{s=p_i}
\end{displaymath}
Ottenendo
\begin{displaymath}
    f(t) = \sum_{i=1}^n k_i e^{p_it}
\end{displaymath}
\subsection{Trasformate notevoli}
Citiamo infine altre due trasformate notevoli:
\begin{displaymath}
    \mathcal{L}[t^n] = \frac{n!}{s^{n+1}} \hspace{10px} \mathcal{L}[e^{\alpha t}] = \frac{1}{s-a}
\end{displaymath}
\end{document}
